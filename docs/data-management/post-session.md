## Within 48h after the FIRST session
!!! danger "Anatomical images must be screened for incidental findings within 48h after the first session"
    
    - [ ] Send the T1-weighted and T2-weighted scan to {{ secrets.people.medical_contact | default("███") }} for screening and incidental findings.
    - [ ] Indicate on [our recruits spreadsheet]({{ secrets.data.recruits_url | default("/redacted.html") }}) that the participant's first session has been submitted for screening.
    - [ ] Wait for response from {{ secrets.people.medical_contact | default("███") }} and note down the result of the screening in our [our recruits spreadsheet]({{ secrets.data.recruits_url | default("/redacted.html") }}).

To do so, you'll need to first [download the data from PACS](#download-the-data-from-the-pacs-with-pacsman-only-authorized-users) and then [convert the data into BIDS](#convert-data-to-bids-with-heudiconv-and-phys2bids) as indicated below.

!!! warning "What to do when there are incidental findings"

    - [ ] Discuss with {{ secrets.people.medical_contact | default("███") }} how to proceed with the participant.
    - [ ] Exclude the participant from the study if {{ secrets.people.medical_contact | default("███") }} evaluates they don't meet the participation (inclusion and exclusion) criteria.

## Within one week after the completed session

### Download the data from the PACS with *PACSMAN* (only authorized users)

- [ ] Log-in into the *PACSMAN* computer  (*{{ secrets.hosts.pacsman }}*)
- [ ] Mount a remote filesystem through sshfs:
    ``` bash
    sshfs {{ secrets.hosts.oesteban | default("<hostname>") }}:{{ settings.paths.pilot_sourcedata }} \
                   $HOME/data/hcph-pilot \
          {{ secrets.data.scp_args | default("<args>") }}
    ```
- [ ] Edit the query file `vim $HOME/queries/last-session.csv` (most likely, just update with the session's date)
``` text title="mydata-onesession.csv"
{% include 'code/pacsman/mydata-onesession.csv' %}
```
- [ ] Prepare and run PACSMAN, pointing the output to the mounted directory.
    ``` bash
    pacsman --save -q $HOME/queries/last-session.csv \
           --out_directory $HOME/data/hcph-pilot/ \
           --config /opt/PACSMAN/files/config.json
    ```
- [ ] Remove write permissions on the newly downloaded data:
    ``` bash
    chmod -R a-w $HOME/data/hcph-pilot/sub-{{ secrets.ids.pacs_subject | default("01") }}/ses-*
    ```
- [ ] Unmount the remote filesystem:
    ``` bash
    sudo umount $HOME/data/hcph-pilot
    ```

### Retrieve physiological recordings (from {{ secrets.hosts.acqknowledge | default("████") }})

### Copy original DICOM datasets into the archive of Stockage hOrus

- [ ] Setup a cron job to execute automatically the synchronization:

    ```
    crontab -e
    [ within your file editor add the following line ]
    0 2 * * * rsync -avurP {{ settings.paths.pilot_sourcedata }}* {{ secrets.data.curnagl_backup | default("<user>@<host>:<path>") }}/sourcedata-pilot &> $HOME/var/log/data-curnagl.log
    ```

## Within two weeks after the completed session

### Convert data to BIDS with *HeuDiConv*

We use *HeuDiConv* to convert from the DICOM format generated by the scanner.
In addition, starting from the piloting session five, we abide by *ReproIn conventions*.
To support backward compatibility (and some extra, currently unsupported features by the original heuristic file), we have our own heuristic file.

??? info "Our custom heuristic file"

    Our heuristic file largely derives from [*ReproIn*'s at the time of writing](https://github.com/nipy/heudiconv/blob/55524168b02519bbf0a3a1c94cafb29a419728a0/heudiconv/heuristics/reproin.py).
    The heuristic has a a `#!python protocols2fix: dict[str | re.Pattern[str], list[tuple[str, str]]]` ([lines 113-148](#__codelineno-6-113)), where replacement patterns to permit backward compatibility are written.

    ``` py linenums="1" hl_lines="113-148"
{% filter indent(width=4) %}
{% include 'code/heudiconv/heuristic_reproin.py' %}
{% endfilter %}
    ```

!!! warning "During piloting, we changed a number of settings"

    For example, the first four sessions did not follow *Reproin* conventions and filenames
    varied substantially.
    Please note the `protocols2fix` variable in our heuristic file, where the compatibility is implemented.

- [ ] Run *HeuDiConv* with our heuristic file `{{ secrets.data.sops_clone_path | default('<path>') }}/code/heudiconv/heuristic_reproin.py`:

    ``` bash title="Executing HeuDiConv"
{% filter indent(width=4) %}
{% include 'code/heudiconv/reproin.sh' %}
{% endfilter %}
    ```

    !!! warning "Session number MUST be updated manually"

    ??? important "Example of the dataset organization"

        Piloting sessions 15 and 16 look like this:

        ``` text
{% filter indent(width=8) %}
{% include 'code/bids/example01.txt' %}
{% endfilter %}
        ```

    ??? warning "We started to generate phase and magnitude only after session 15"

        As a result, the piloting data up to session 14 will look more like:

        ``` text
{% filter indent(width=8) %}
{% include 'code/bids/example02.txt' %}
{% endfilter %}
        ```

### Convert physiological recordings and eye-tracking data to BIDS

- [ ] Plot an overview of the data with the following command.
     This command generates a PNG plot of the data within the current directory without processing the data itself.
     The physiological data folder is specified via the `-in` command line argument.
    ``` shell
    phys2bids -in {{ settings.paths.pilot_sourcedata }}/physio/session-recording.acq -info
    ```
- [ ] Check that all the channels are present in the PNG plot.
- [ ] If this is the case, proceed to process the file using the subsequent command.
    Use the `-ntp` argument to specify the number of volumes for each task, and the `-tr` argument to indicate the task's repetition time.
    Define the output directory with `-outdir` and provide the path to the heuristic file using `-heur`.
    Adjust the subject and session numbers accordingly.
    Should scanner trigger transmission encounter issues and manual adjustments are made to the trigger data, it is possible to allocate one trigger per task.
    Set the repetition time duration as the task length, as demonstrated in the example below.
    ``` shell
    phys2bids -in modified-last-session_multiscan.txt -chtrig 4 -ntp  1 1 1 -tr 158 1200 331 -thr 2 -outdir outputdir -heur heur_physio.py -sub pilot -ses pilot016
    ```

- [ ] Execute the script `write_event_file.py` as shown below to generate task event files.
    This script creates JSON and TSV files containing event information and generates PNG plots for each task, displaying both physiological data and corresponding events.
    These plots are saved in the current directory.
    The script must be executed with the following command, where `outputdir` is the output directory of *phys2bids*:
    ``` shell
    python write_event_file.py --path ./outputdir/sub-001/ses-pilot016/func/
    ```

Once the script is executed, the BIDS folder (consisting solely of physiological data in this case) will have the following structure:
```
ses-pilot016
    └── func
        ├── sub-001_ses-pilot016_task-bht_events.json
        ├── sub-001_ses-pilot016_task-bht_events.tsv
        ├── sub-001_ses-pilot016_task-bht_physio.json
        ├── sub-001_ses-pilot016_task-bht_physio.tsv.gz
        ├── sub-001_ses-pilot016_task-qct_events.json
        ├── sub-001_ses-pilot016_task-qct_events.tsv
        ├── sub-001_ses-pilot016_task-qct_physio.json
        ├── sub-001_ses-pilot016_task-qct_physio.tsv.gz
        ├── sub-001_ses-pilot016_task-rest_events.json
        ├── sub-001_ses-pilot016_task-rest_events.tsv
        ├── sub-001_ses-pilot016_task-rest_physio.json
        └── sub-001_ses-pilot016_task-rest_physio.tsv.gz
```

### Add new data to the *DataLad* dataset

As new sessions are collected, the corresponding BIDS structures MUST be saved within the *DataLad* dataset and pushed to remote storage systems:

- [ ] Save the files in the dataset history using the command below.
    Replace `<session_id>` below with the number of the session (e.g., `pilot017`):
    ``` shell
    datalad save -r -m "add: session <session_id>" sub-001/ses-<session_id>
    ```
- [ ] Push the new data to the remote storage (if your git containing *DataLad* and the Git annex is different from `origin`, e.g., `github`, replace the name below):
    ``` shell
    datalad push --to origin
    ```
